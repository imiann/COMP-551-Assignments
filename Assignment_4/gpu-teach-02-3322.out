Loading model...
Model loaded successfully.
Loading tokenizer...
Tokenizing inputs...
Tokenized inputs: {'input_ids': tensor([[  101,  1996,  3185,  2001, 10392,  1998,  2440,  1997, 20096,  1012,
           102,  1045, 12246,  5632,  1996,  2143,  1012,   102]],
       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]],
       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
       device='cuda:0')}
Running forward pass to compute attention...
Attention computed successfully.
Attention Shape: [torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18]), torch.Size([1, 12, 18, 18])]
Sample Attention Weights (Layer 0, Head 0):
[[0.03748975 0.08512127 0.01887494 0.02609591 0.022048   0.07028334
  0.04654152 0.05372543 0.03237069 0.07704775 0.19246691 0.02695502
  0.03634614 0.02008027 0.06457709 0.02423933 0.05460497 0.1111317 ]
 [0.09462734 0.10125855 0.05289515 0.09645701 0.10275403 0.06060085
  0.07418814 0.08282474 0.11232388 0.1037305  0.09185281 0.00408629
  0.00383849 0.0042465  0.00400271 0.0035996  0.00385437 0.00285903]
 [0.03659862 0.06588151 0.096821   0.0661115  0.16057254 0.10956734
  0.0747786  0.07407156 0.09369626 0.09273699 0.11137402 0.00430884
  0.00264956 0.0033672  0.00128937 0.00247165 0.001895   0.00180846]
 [0.11386018 0.0357285  0.04683751 0.19713536 0.08321734 0.0643942
  0.07575972 0.05592962 0.16877033 0.0890004  0.04463551 0.00399599
  0.00410943 0.00591994 0.00162702 0.00361601 0.00380062 0.00166237]
 [0.07983593 0.02124016 0.14351808 0.08144327 0.06639907 0.10057347
  0.03121649 0.04750114 0.18210389 0.16000666 0.07197462 0.0016673
  0.00189005 0.00234411 0.00037425 0.00278115 0.00377167 0.0013587 ]
 [0.0881732  0.05874342 0.05727686 0.09756402 0.07329683 0.10943511
  0.05576011 0.12199743 0.11803315 0.10158528 0.09212592 0.00298141
  0.00398988 0.00476285 0.00252269 0.00333354 0.00450237 0.00391591]
 [0.08500201 0.02431995 0.08505725 0.09347592 0.0787269  0.05923558
  0.03059123 0.09359826 0.21344131 0.06948299 0.14058551 0.00448602
  0.00293756 0.00506544 0.00100195 0.00361481 0.00355062 0.00582665]
 [0.10698182 0.06158721 0.06192289 0.10555875 0.08297648 0.10618529
  0.07752351 0.10575717 0.0948243  0.08709095 0.09028921 0.00284446
  0.00245829 0.00347645 0.00210052 0.00286486 0.0028293  0.00272853]
 [0.09212257 0.02628398 0.09787901 0.10330447 0.12901337 0.09385607
  0.04217255 0.05200468 0.14537026 0.15909168 0.03733255 0.00383825
  0.00328667 0.00537153 0.00067888 0.00317991 0.00434971 0.0008639 ]
 [0.07072796 0.07314333 0.07606068 0.08415338 0.07289717 0.10133898
  0.07823808 0.07860617 0.10399403 0.13731627 0.10288092 0.00211268
  0.0029594  0.00297405 0.00217903 0.00280214 0.00454793 0.00306781]
 [0.09272502 0.09552151 0.05705811 0.05592391 0.07359596 0.09135824
  0.03959292 0.07593566 0.09421558 0.15462324 0.14307918 0.00346061
  0.00294313 0.00277633 0.0037963  0.00316864 0.0057218  0.00450388]
 [0.12509626 0.0009985  0.00293942 0.00420533 0.00451828 0.0012969
  0.00233221 0.00124939 0.0054369  0.00173728 0.00318968 0.1414304
  0.16932917 0.21171948 0.03345436 0.09822888 0.05885528 0.1339823 ]
 [0.08222456 0.00055859 0.00437196 0.00437171 0.00333928 0.00483852
  0.00210338 0.00184638 0.00535006 0.00277184 0.00774364 0.08988357
  0.10340043 0.17397583 0.01638411 0.13066722 0.09245854 0.27371034]
 [0.06585197 0.00319705 0.00625298 0.01134892 0.00688303 0.00276873
  0.0049546  0.00466993 0.01192681 0.00411407 0.0083426  0.12367121
  0.2019587  0.08986072 0.06096787 0.12251928 0.08827115 0.18244039]
 [0.07034884 0.00372945 0.00197329 0.00400235 0.00373027 0.00179154
  0.00284469 0.00339126 0.00574223 0.0029651  0.0039273  0.14051436
  0.16401881 0.13248526 0.12141042 0.09950732 0.09745131 0.14016621]
 [0.08257601 0.00158362 0.00741957 0.00516344 0.00555877 0.00287822
  0.00290586 0.00134641 0.00552157 0.00223245 0.00293752 0.1512192
  0.16337468 0.24410228 0.03899581 0.14431609 0.0508279  0.08704061]
 [0.06822275 0.00279167 0.00253082 0.00391259 0.00315514 0.0042704
  0.00385691 0.00290148 0.00424446 0.00437117 0.00525311 0.08483591
  0.1764732  0.11646418 0.09244879 0.09106584 0.1331005  0.2001011 ]
 [0.04503464 0.00203673 0.0011674  0.00135903 0.00178825 0.00204131
  0.00103478 0.00140834 0.0023333  0.002811   0.00456197 0.11941184
  0.14733158 0.08438858 0.11950749 0.09339789 0.13641956 0.23396622]]
Tokens: ['[CLS]', 'the', 'movie', 'was', 'fantastic', 'and', 'full', 'of', 'surprises', '.', '[SEP]', 'i', 'thoroughly', 'enjoyed', 'the', 'film', '.', '[SEP]']
Sentence B start index: 11
Generating head view visualization...
<IPython.core.display.HTML object>
<IPython.core.display.HTML object>
<IPython.core.display.Javascript object>
An error occurred during BERTViz testing: head_view returned None, visualization could not be generated.
Falling back to manual attention visualization...
Visualizing attention for Layer 0, Head 0...
Manual attention visualization displayed successfully.
