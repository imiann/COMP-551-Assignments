{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qebXK4szqiZq",
    "outputId": "883d3efd-65ff-42ff-8a94-b2984e747aea"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "UK2_rmLqqJNw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "oD1jVBrAqneI"
   },
   "outputs": [],
   "source": [
    "# Load the simplified version of GoEmotions\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "# Access train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKadZFvFqsRM",
    "outputId": "d42d40fe-dab7-4905-9bd1-46958b22e86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43410 entries, 0 to 43409\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    43410 non-null  object\n",
      " 1   labels  43410 non-null  object\n",
      " 2   id      43410 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1017.5+ KB\n",
      "\n",
      "Sample Data:                                                 text labels       id\n",
      "0  My favourite food is anything I didn't have to...   [27]  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...   [27]  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj\n",
      "3                        To make her feel threatened   [14]  ed7ypvh\n",
      "4                             Dirty Southern Wankers    [3]  ed0bdzj \n",
      "\n",
      "\n",
      "\n",
      "Validation Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5426 entries, 0 to 5425\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5426 non-null   object\n",
      " 1   labels  5426 non-null   object\n",
      " 2   id      5426 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 127.3+ KB\n",
      "\n",
      "Sample Data:                                                 text   labels       id\n",
      "0  Is this in New Orleans?? I really feel like th...     [27]  edgurhb\n",
      "1  You know the answer man, you are programmed to...  [4, 27]  ee84bjg\n",
      "2               I've never been this sad in my life!     [25]  edcu99z\n",
      "3  The economy is heavily controlled and subsidiz...  [4, 27]  edc32e2\n",
      "4  He could have easily taken a real camera from ...     [20]  eepig6r \n",
      "\n",
      "\n",
      "\n",
      "Testing Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5427 entries, 0 to 5426\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5427 non-null   object\n",
      " 1   labels  5427 non-null   object\n",
      " 2   id      5427 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 127.3+ KB\n",
      "\n",
      "Sample Data:                                                 text labels       id\n",
      "0  I’m really sorry about your situation :( Altho...   [25]  eecwqtt\n",
      "1    It's wonderful because it's awful. At not with.    [0]  ed5f85d\n",
      "2  Kings fan here, good luck to you guys! Will be...   [13]  een27c3\n",
      "3  I didn't know that, thank you for teaching me ...   [15]  eelgwd1\n",
      "4  They got bored from haunting earth for thousan...   [27]  eem5uti \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas DataFrames\n",
    "train_df = train_data.to_pandas()\n",
    "validation_df = validation_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(\"Training Data Info:\")\n",
    "train_df.info()\n",
    "print(\"\\nSample Data:\", train_df.head(), \"\\n\\n\\n\")\n",
    "\n",
    "print(\"Validation Data Info:\")\n",
    "validation_df.info()\n",
    "print(\"\\nSample Data:\", validation_df.head(), \"\\n\\n\\n\")\n",
    "\n",
    "print(\"Testing Data Info:\")\n",
    "test_df.info()\n",
    "print(\"\\nSample Data:\", test_df.head(), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "806mUtcpr3uF",
    "outputId": "e2310d01-bd36-49cb-91cf-7f1f33e80ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "\n",
      "Training data frame: \n",
      " text      0\n",
      "labels    0\n",
      "id        0\n",
      "dtype: int64\n",
      "\n",
      "Validation data frame: \n",
      " text      0\n",
      "labels    0\n",
      "id        0\n",
      "dtype: int64\n",
      "\n",
      "Testing data frame: \n",
      " text      0\n",
      "labels    0\n",
      "id        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"Missing Values:\")\n",
    "print(\"\\nTraining data frame: \\n\", train_df.isnull().sum())\n",
    "print(\"\\nValidation data frame: \\n\", train_df.isnull().sum())\n",
    "print(\"\\nTesting data frame: \\n\", train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPvifbLAsEzW",
    "outputId": "f99eb165-e322-4388-e3e4-8adfbf306d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Training Data Shape: (36308, 3)\n",
      "\n",
      "Filtered Validation Data Shape: (4548, 3)\n",
      "\n",
      "Filtered Testing Data Shape: (4590, 3)\n",
      "\n",
      "Sample from Training Data:\n",
      "                                                 text  labels       id\n",
      "0  My favourite food is anything I didn't have to...      27  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...      27  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING       2  eezlygj\n",
      "3                        To make her feel threatened      14  ed7ypvh\n",
      "4                             Dirty Southern Wankers       3  ed0bdzj\n",
      "\n",
      "Sample from Validation Data:\n",
      "                                                 text  labels       id\n",
      "0  Is this in New Orleans?? I really feel like th...      27  edgurhb\n",
      "2               I've never been this sad in my life!      25  edcu99z\n",
      "4  He could have easily taken a real camera from ...      20  eepig6r\n",
      "5  Thank you for your vote of confidence, but we ...      15  eczm50f\n",
      "6  Wah Mum other people call me on my bullshit an...       2  ed4yr9r\n",
      "\n",
      "Sample from Test Data:\n",
      "                                                 text  labels       id\n",
      "0  I’m really sorry about your situation :( Altho...      25  eecwqtt\n",
      "1    It's wonderful because it's awful. At not with.       0  ed5f85d\n",
      "2  Kings fan here, good luck to you guys! Will be...      13  een27c3\n",
      "3  I didn't know that, thank you for teaching me ...      15  eelgwd1\n",
      "4  They got bored from haunting earth for thousan...      27  eem5uti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\finnl_y\\AppData\\Local\\Temp\\ipykernel_4936\\1964841269.py:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  train_df.loc[:, 'labels'] = train_df['labels'].apply(extract_label)\n",
      "C:\\Users\\finnl_y\\AppData\\Local\\Temp\\ipykernel_4936\\1964841269.py:12: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  validation_df.loc[:, 'labels'] = validation_df['labels'].apply(extract_label)\n",
      "C:\\Users\\finnl_y\\AppData\\Local\\Temp\\ipykernel_4936\\1964841269.py:15: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  test_df.loc[:, 'labels'] = test_df['labels'].apply(extract_label)\n"
     ]
    }
   ],
   "source": [
    "# Delete multiple labels\n",
    "\n",
    "# Define a helper function to extract the single label\n",
    "def extract_label(label_list):\n",
    "  return label_list[0]\n",
    "\n",
    "# Filter rows with exactly one label and extract the single label\n",
    "train_df = train_df[train_df['labels'].apply(len) == 1]\n",
    "train_df.loc[:, 'labels'] = train_df['labels'].apply(extract_label)\n",
    "\n",
    "validation_df = validation_df[validation_df['labels'].apply(len) == 1]\n",
    "validation_df.loc[:, 'labels'] = validation_df['labels'].apply(extract_label)\n",
    "\n",
    "test_df = test_df[test_df['labels'].apply(len) == 1]\n",
    "test_df.loc[:, 'labels'] = test_df['labels'].apply(extract_label)\n",
    "\n",
    "# Check shapes after filtering\n",
    "print(\"Filtered Training Data Shape:\", train_df.shape)\n",
    "print(\"\\nFiltered Validation Data Shape:\", validation_df.shape)\n",
    "print(\"\\nFiltered Testing Data Shape:\", test_df.shape)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nSample from Training Data:\\n\", train_df.head())\n",
    "\n",
    "print(\"\\nSample from Validation Data:\\n\", validation_df.head())\n",
    "\n",
    "print(\"\\nSample from Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIy5bzn06gy-",
    "outputId": "c1c9b5c2-dbe8-49e5-837e-ce8e0000ed84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27  2 14  3 26 15  0  6  5 12 17 25 10 20  4 13  1  9 24 18  7 22 11 23\n",
      " 21 16  8 19]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['labels'].unique())  # Show all unique values in the column\n",
    "print(train_df['labels'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aK2hDarR62IV",
    "outputId": "1be59bab-cf2e-4d1a-eb30-6360752bff1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "# Conversion of label objects to Integers\n",
    "train_df['labels'] = train_df['labels'].astype(int)\n",
    "validation_df['labels'] = validation_df['labels'].astype(int)\n",
    "test_df['labels'] = test_df['labels'].astype(int)\n",
    "\n",
    "# Confirm the data type\n",
    "print(train_df['labels'].dtype)  # Should now show 'int64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sXEZuXb7JqK",
    "outputId": "f2bab71e-d354-4308-a082-d029b8b2d43d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Training Data Shape: (36308, 3)\n",
      "\n",
      "Filtered Validation Data Shape: (4548, 3)\n",
      "\n",
      "Filtered Testing Data Shape: (4590, 3)\n",
      "\n",
      "Sample from Training Data:\n",
      "                                                 text  labels       id\n",
      "0  My favourite food is anything I didn't have to...      27  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...      27  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING       2  eezlygj\n",
      "3                        To make her feel threatened      14  ed7ypvh\n",
      "4                             Dirty Southern Wankers       3  ed0bdzj\n",
      "\n",
      "Sample from Validation Data:\n",
      "                                                 text  labels       id\n",
      "0  Is this in New Orleans?? I really feel like th...      27  edgurhb\n",
      "2               I've never been this sad in my life!      25  edcu99z\n",
      "4  He could have easily taken a real camera from ...      20  eepig6r\n",
      "5  Thank you for your vote of confidence, but we ...      15  eczm50f\n",
      "6  Wah Mum other people call me on my bullshit an...       2  ed4yr9r\n",
      "\n",
      "Sample from Test Data:\n",
      "                                                 text  labels       id\n",
      "0  I’m really sorry about your situation :( Altho...      25  eecwqtt\n",
      "1    It's wonderful because it's awful. At not with.       0  ed5f85d\n",
      "2  Kings fan here, good luck to you guys! Will be...      13  een27c3\n",
      "3  I didn't know that, thank you for teaching me ...      15  eelgwd1\n",
      "4  They got bored from haunting earth for thousan...      27  eem5uti\n"
     ]
    }
   ],
   "source": [
    "# Final check\n",
    "# Check shapes after filtering\n",
    "print(\"Filtered Training Data Shape:\", train_df.shape)\n",
    "print(\"\\nFiltered Validation Data Shape:\", validation_df.shape)\n",
    "print(\"\\nFiltered Testing Data Shape:\", test_df.shape)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nSample from Training Data:\\n\", train_df.head())\n",
    "\n",
    "print(\"\\nSample from Validation Data:\\n\", validation_df.head())\n",
    "\n",
    "print(\"\\nSample from Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-hZo9fywtpC",
    "outputId": "360fbe21-1f9d-4447-ec43-d7ed0c568a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training Feature Matrix Shape: (36308, 24311)\n",
      "Baseline Validation Feature Matrix Shape: (4548, 24311)\n",
      "Baseline Testing Feature Matrix Shape: (4590, 24311)\n",
      "Baseline Training label Matrix Shape: (36308,)\n",
      "Baseline Validation label Matrix Shape: (4548,)\n",
      "Baseline Testing label Matrix Shape: (4590,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for Baseline\n",
    "\n",
    "# Initialize a Vectorizer\n",
    "#vectorizer_baseline = CountVectorizer()\n",
    "vectorizer_baseline = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on training data and transform all splits\n",
    "X_train_baseline = vectorizer_baseline.fit_transform(train_df['text'])\n",
    "X_validation_baseline = vectorizer_baseline.transform(validation_df['text'])\n",
    "X_test_baseline = vectorizer_baseline.transform(test_df['text'])\n",
    "\n",
    "# Labels (already preprocessed)\n",
    "y_train_baseline = train_df['labels'].to_numpy()  # Convert to NumPy array\n",
    "y_validation_baseline = validation_df['labels'].to_numpy()\n",
    "y_test_baseline = test_df['labels'].to_numpy()\n",
    "\n",
    "# Verify feature matrix shapes\n",
    "print(\"Baseline Training Feature Matrix Shape:\", X_train_baseline.shape)\n",
    "print(\"Baseline Validation Feature Matrix Shape:\", X_validation_baseline.shape)\n",
    "print(\"Baseline Testing Feature Matrix Shape:\", X_test_baseline.shape)\n",
    "print(\"Baseline Training label Matrix Shape:\", y_train_baseline.shape)\n",
    "print(\"Baseline Validation label Matrix Shape:\", y_validation_baseline.shape)\n",
    "print(\"Baseline Testing label Matrix Shape:\", y_test_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va0hr5JV2ry2",
    "outputId": "edede2cb-b5b2-4c61-95f1-7f1459d5868a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Feature Matrix Shape: (36308, 24311)\n",
      "Naive Bayes Validation Feature Matrix Shape: (4548, 24311)\n",
      "Naive Bayes Testing Feature Matrix Shape: (4590, 24311)\n",
      "Naive Bayes Training label Matrix Shape: (36308,)\n",
      "Naive Bayes Validation label Matrix Shape: (4548,)\n",
      "Naive Bayes Testing label Matrix Shape: (4590,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for Bayesian\n",
    "\n",
    "# Initialize a Vectorizer\n",
    "vectorizer_bayesian = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on training data and transform all splits\n",
    "X_train_bayesian = vectorizer_bayesian.fit_transform(train_df['text'])\n",
    "X_validation_bayesian = vectorizer_bayesian.transform(validation_df['text'])\n",
    "X_test_bayesian = vectorizer_bayesian.transform(test_df['text'])\n",
    "\n",
    "# Labels (already preprocessed)\n",
    "y_train_bayesian = train_df['labels'].to_numpy()  # Convert to NumPy array\n",
    "y_validation_bayesian = validation_df['labels'].to_numpy()\n",
    "y_test_bayesian = test_df['labels'].to_numpy()\n",
    "\n",
    "# Verify feature matrix shapes\n",
    "print(\"Naive Bayes Training Feature Matrix Shape:\", X_train_bayesian.shape)\n",
    "print(\"Naive Bayes Validation Feature Matrix Shape:\", X_validation_bayesian.shape)\n",
    "print(\"Naive Bayes Testing Feature Matrix Shape:\", X_test_bayesian.shape)\n",
    "print(\"Naive Bayes Training label Matrix Shape:\", y_train_bayesian.shape)\n",
    "print(\"Naive Bayes Validation label Matrix Shape:\", y_validation_bayesian.shape)\n",
    "print(\"Naive Bayes Testing label Matrix Shape:\", y_test_bayesian.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6jXPOP1ysA4",
    "outputId": "c6c9765f-63cf-43aa-d31c-e50b0c12e16f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution: \n",
      "     Class Label  Count  Frequency (%)\n",
      "0             0   2710       7.463920\n",
      "1             1   1652       4.549961\n",
      "2             2   1025       2.823069\n",
      "3             3   1451       3.996364\n",
      "4             4   1873       5.158643\n",
      "5             5    649       1.787485\n",
      "6             6    858       2.363116\n",
      "7             7   1389       3.825603\n",
      "8             8    389       1.071389\n",
      "9             9    709       1.952738\n",
      "10           10   1402       3.861408\n",
      "11           11    498       1.371599\n",
      "12           12    203       0.559105\n",
      "13           13    510       1.404649\n",
      "14           14    430       1.184312\n",
      "15           15   1857       5.114575\n",
      "16           16     39       0.107414\n",
      "17           17    853       2.349344\n",
      "18           18   1427       3.930263\n",
      "19           19     85       0.234108\n",
      "20           20    861       2.371378\n",
      "21           21     51       0.140465\n",
      "22           22    586       1.613969\n",
      "23           23     88       0.242371\n",
      "24           24    353       0.972238\n",
      "25           25    817       2.250193\n",
      "26           26    720       1.983034\n",
      "27           27  12823      35.317285\n",
      "\n",
      "Validation Label Distribution:\n",
      "     Class Label  Count  Frequency (%)\n",
      "0             0    326       7.167986\n",
      "1             1    208       4.573439\n",
      "2             2    109       2.396658\n",
      "3             3    164       3.605981\n",
      "4             4    258       5.672823\n",
      "5             5     96       2.110818\n",
      "6             6    102       2.242744\n",
      "7             7    164       3.605981\n",
      "8             8     52       1.143360\n",
      "9             9     91       2.000880\n",
      "10           10    212       4.661390\n",
      "11           11     61       1.341249\n",
      "12           12     20       0.439754\n",
      "13           13     52       1.143360\n",
      "14           14     58       1.275286\n",
      "15           15    261       5.738786\n",
      "16           16      6       0.131926\n",
      "17           17    106       2.330695\n",
      "18           18    173       3.803870\n",
      "19           19      8       0.175901\n",
      "20           20    119       2.616535\n",
      "21           21      9       0.197889\n",
      "22           22     74       1.627089\n",
      "23           23      8       0.175901\n",
      "24           24     40       0.879507\n",
      "25           25     84       1.846966\n",
      "26           26     95       2.088830\n",
      "27           27   1592      35.004398\n",
      "\n",
      "Testing Label Distribution:\n",
      "     Class Label  Count  Frequency (%)\n",
      "0             0    348       7.581699\n",
      "1             1    186       4.052288\n",
      "2             2    131       2.854031\n",
      "3             3    194       4.226580\n",
      "4             4    236       5.141612\n",
      "5             5     86       1.873638\n",
      "6             6     97       2.113290\n",
      "7             7    176       3.834423\n",
      "8             8     56       1.220044\n",
      "9             9     88       1.917211\n",
      "10           10    195       4.248366\n",
      "11           11     76       1.655773\n",
      "12           12     23       0.501089\n",
      "13           13     57       1.241830\n",
      "14           14     65       1.416122\n",
      "15           15    260       5.664488\n",
      "16           16      2       0.043573\n",
      "17           17     93       2.026144\n",
      "18           18    160       3.485839\n",
      "19           19     12       0.261438\n",
      "20           20    107       2.331155\n",
      "21           21      7       0.152505\n",
      "22           22     89       1.938998\n",
      "23           23      7       0.152505\n",
      "24           24     44       0.958606\n",
      "25           25    102       2.222222\n",
      "26           26     87       1.895425\n",
      "27           27   1606      34.989107\n"
     ]
    }
   ],
   "source": [
    "# Class distributions\n",
    "\n",
    "def class_distribtion(label_set):\n",
    "  labels, counts = np.unique(label_set, return_counts=True)\n",
    "\n",
    "  # Calculate frequencies as percentages\n",
    "  frequencies = (counts / counts.sum()) * 100\n",
    "\n",
    "  # Create a DataFrame to store counts and frequencies\n",
    "  class_distribution_table = pd.DataFrame({\n",
    "    \"Class Label\": labels,\n",
    "    \"Count\": counts,\n",
    "    \"Frequency (%)\": frequencies\n",
    "  })\n",
    "\n",
    "  # Sort the table by class label for better organization\n",
    "  class_distribution_table = class_distribution_table.sort_values(by=\"Class Label\").reset_index(drop=True)\n",
    "  return class_distribution_table\n",
    "\n",
    "print(\"Training Label Distribution: \\n\", class_distribtion(y_train_bayesian))\n",
    "print(\"\\nValidation Label Distribution:\\n\", class_distribtion(y_validation_bayesian))\n",
    "print(\"\\nTesting Label Distribution:\\n\", class_distribtion(y_test_bayesian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIy6Zvif3iwz",
    "outputId": "8051f9e1-af3b-484a-adbb-68283095798f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Input IDs Shape: torch.Size([36308, 128])\n",
      "Training Attention Masks Shape: torch.Size([36308, 128])\n",
      "Training Labels Shape: torch.Size([36308])\n",
      "Validation Input IDs Shape: torch.Size([4548, 128])\n",
      "Validation Attention Masks Shape: torch.Size([4548, 128])\n",
      "Validation Labels Shape: torch.Size([4548])\n",
      "Testing Input IDs Shape: torch.Size([4590, 128])\n",
      "Testing Attention Masks Shape: torch.Size([4590, 128])\n",
      "Testing Labels Shape: torch.Size([4590])\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for LLM\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # We will use bert-base-uncased\n",
    "\n",
    "# Tokenize text data for all splits\n",
    "train_encodings = tokenizer_llm(\n",
    "    list(train_df['text']), truncation=True, padding='max_length', max_length=128\n",
    ")\n",
    "validation_encodings = tokenizer_llm(\n",
    "    list(validation_df['text']), truncation=True, padding='max_length', max_length=128\n",
    ")\n",
    "test_encodings = tokenizer_llm(\n",
    "    list(test_df['text']), truncation=True, padding='max_length', max_length=128\n",
    ")\n",
    "\n",
    "# Convert tokenized data into PyTorch tensors\n",
    "X_train_llm = torch.tensor(train_encodings['input_ids'])\n",
    "X_train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "y_train_llm = torch.tensor(train_df['labels'].to_numpy())\n",
    "\n",
    "X_validation_llm = torch.tensor(validation_encodings['input_ids'])\n",
    "X_validation_attention_masks = torch.tensor(validation_encodings['attention_mask'])\n",
    "y_validation_llm = torch.tensor(validation_df['labels'].to_numpy())\n",
    "\n",
    "X_test_llm = torch.tensor(test_encodings['input_ids'])\n",
    "X_test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "y_test_llm = torch.tensor(test_df['labels'].to_numpy())\n",
    "\n",
    "# Check shapes\n",
    "print(\"Training Input IDs Shape:\", X_train_llm.shape)\n",
    "print(\"Training Attention Masks Shape:\", X_train_attention_masks.shape)\n",
    "print(\"Training Labels Shape:\", y_train_llm.shape)\n",
    "\n",
    "print(\"Validation Input IDs Shape:\", X_validation_llm.shape)\n",
    "print(\"Validation Attention Masks Shape:\", X_validation_attention_masks.shape)\n",
    "print(\"Validation Labels Shape:\", y_validation_llm.shape)\n",
    "\n",
    "print(\"Testing Input IDs Shape:\", X_test_llm.shape)\n",
    "print(\"Testing Attention Masks Shape:\", X_test_attention_masks.shape)\n",
    "print(\"Testing Labels Shape:\", y_test_llm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Implement Naive Bayes and Finetune an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 - Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(Z):                                                # dimension C x N\n",
    "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
    "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
    "    return log_sum_exp\n",
    "\n",
    "class NaiveBayes: \n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        N, D = x.shape\n",
    "        C = np.max(y) + 1\n",
    "        # one parameter for each feature conditioned on each class\n",
    "        mu, sigma = np.zeros((C,D)), np.zeros((C,D))\n",
    "        Nc = np.zeros(C) # number of instances in class c\n",
    "        # for each class get the MLE for the mean and std\n",
    "        for c in range(C):\n",
    "            x_c = x[y == c]                           #slice all the elements from class c\n",
    "            Nc[c] = x_c.shape[0]                      #get number of elements of class c\n",
    "            mu[c,:] = np.mean(x_c,0)                  #mean of features of class c\n",
    "            sigma[c,:] = np.std(x_c, 0)               #std of features of class c\n",
    "            \n",
    "        self.mu = mu                                  # C x D\n",
    "        self.sigma = sigma                            # C x D\n",
    "        self.pi = (Nc+1)/(N+C)                        #Laplace smoothing (using alpha_c=1 for all c) you can derive using Dirichlet's distribution\n",
    "        return self\n",
    "\n",
    "    def predict(self, xt):\n",
    "        Nt, D = xt.shape\n",
    "        # for numerical stability we work in the log domain\n",
    "        # we add a dimension because this is added to the log-likelihood matrix \n",
    "        # that assigns a likelihood for each class (C) to each test point, and so it is C x N\n",
    "        log_prior = np.log(self.pi)[:, None]\n",
    "        # logarithm of the likelihood term for Gaussian \n",
    "        # the first two terms are the logarithm of the normalization term in the Gaussian and the final term is the exponent in the Gaussian. \n",
    "        # Notice that we are adding dimensions (using None) to model parameters and data to make this evaluation. \n",
    "        # The reason is that sigma and mu are C x D, while the data x is N x D. We operate on a C x N x D shape by increasing the number of dimensions when needed\n",
    "        log_likelihood = -.5 * np.log(2*np.pi) - np.log(self.sigma[:,None,:]) -.5 * (((xt[None,:,:] - self.mu[:,None,:])/self.sigma[:,None,:])**2)\n",
    "        # now we sum over the feature dimension to get a C x N matrix (this has the log-likelihood for each class-test point combination)\n",
    "        log_likelihood = np.sum(log_likelihood, axis=2)\n",
    "        # posterior calculation\n",
    "        log_posterior = log_prior + log_likelihood\n",
    "        posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
    "        return posterior.T                                                  # dimension N x C\n",
    "    \n",
    "    def evaluate_acc(self, y, y_hat):\n",
    "        return np.mean(y == y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesOptimized:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.feature_likelihoods = {}\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Calculate prior probabilities\n",
    "        for cls in self.classes:\n",
    "            X_cls = X[y == cls]\n",
    "            self.class_priors[cls] = X_cls.shape[0] / n_samples\n",
    "            self.feature_likelihoods[cls] = {}\n",
    "            \n",
    "            # Calculate likelihoods\n",
    "            for feature_index in range(n_features):\n",
    "                feature_values = X_cls[:, feature_index]\n",
    "                unique_values, counts = np.unique(feature_values, return_counts=True)\n",
    "                likelihoods = counts / counts.sum()\n",
    "                self.feature_likelihoods[cls][feature_index] = dict(zip(unique_values, likelihoods))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        log_probs = np.zeros((n_samples, len(self.classes)))\n",
    "        \n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            log_prob = np.log(self.class_priors[cls])\n",
    "            log_probs[:, idx] = log_prob\n",
    "            \n",
    "            for feature_index in range(n_features):\n",
    "                feature_values = X[:, feature_index]\n",
    "                likelihoods = np.array([self.feature_likelihoods[cls][feature_index].get(value, 1e-6) for value in feature_values])\n",
    "                log_probs[:, idx] += np.log(likelihoods)\n",
    "        \n",
    "        return self.classes[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "    def evaluate_acc(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return correct / len(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = X_train_bayesian[:500]\n",
    "y_train_small = y_train_bayesian[:500]\n",
    "\n",
    "X_validation_small = X_validation_bayesian[:500]\n",
    "y_validation_small = y_validation_bayesian[:500]\n",
    "\n",
    "X_test_small = X_test_bayesian[:500]\n",
    "y_test_small = y_test_bayesian[:500]\n",
    "\n",
    "naive_bayes = NaiveBayesOptimized() \n",
    "naive_bayes.fit(X_train_bayesian.toarray(), y_train_bayesian)\n",
    "y_hat = naive_bayes.predict(X_test_bayesian.toarray())\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = naive_bayes.evaluate_acc(y_test_bayesian, y_hat)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Softmax Regression): 0.5521108179419525\n",
      "Test Accuracy (Softmax Regression): 0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "# 2. Implement Softmax Regression from Sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the model\n",
    "softmax_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model\n",
    "softmax_regression.fit(X_train_baseline, y_train_baseline)\n",
    "\n",
    "# Predict on the validation, test set\n",
    "y_pred_softmax = softmax_regression.predict(X_validation_baseline)\n",
    "y_pred_test_softmax = softmax_regression.predict(X_test_baseline)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_validation_softmax = accuracy_score(y_validation_baseline, y_pred_softmax)\n",
    "accuracy_test_softmax = accuracy_score(y_test_baseline, y_pred_test_softmax)\n",
    "\n",
    "print(\"Validation Accuracy (Softmax Regression):\", accuracy_validation_softmax)\n",
    "print(\"Test Accuracy (Softmax Regression):\", accuracy_test_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "    paths: {\n",
    "        d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n",
    "        jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "    }\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Model\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "\n",
    "# this is the tokenizer using BERT tokenizer_llm\n",
    "\n",
    "model_LLM = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
