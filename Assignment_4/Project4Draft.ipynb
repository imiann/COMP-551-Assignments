{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qebXK4szqiZq",
    "outputId": "883d3efd-65ff-42ff-8a94-b2984e747aea"
   },
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "UK2_rmLqqJNw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "oD1jVBrAqneI"
   },
   "outputs": [],
   "source": [
    "# Load the simplified version of GoEmotions\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "\n",
    "# Access train, validation, and test splits\n",
    "train_data = dataset[\"train\"]\n",
    "validation_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKadZFvFqsRM",
    "outputId": "d42d40fe-dab7-4905-9bd1-46958b22e86a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43410 entries, 0 to 43409\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    43410 non-null  object\n",
      " 1   labels  43410 non-null  object\n",
      " 2   id      43410 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1017.5+ KB\n",
      "\n",
      "Sample Data:                                                 text labels       id\n",
      "0  My favourite food is anything I didn't have to...   [27]  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...   [27]  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj\n",
      "3                        To make her feel threatened   [14]  ed7ypvh\n",
      "4                             Dirty Southern Wankers    [3]  ed0bdzj \n",
      "\n",
      "\n",
      "\n",
      "Validation Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5426 entries, 0 to 5425\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5426 non-null   object\n",
      " 1   labels  5426 non-null   object\n",
      " 2   id      5426 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 127.3+ KB\n",
      "\n",
      "Sample Data:                                                 text   labels       id\n",
      "0  Is this in New Orleans?? I really feel like th...     [27]  edgurhb\n",
      "1  You know the answer man, you are programmed to...  [4, 27]  ee84bjg\n",
      "2               I've never been this sad in my life!     [25]  edcu99z\n",
      "3  The economy is heavily controlled and subsidiz...  [4, 27]  edc32e2\n",
      "4  He could have easily taken a real camera from ...     [20]  eepig6r \n",
      "\n",
      "\n",
      "\n",
      "Testing Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5427 entries, 0 to 5426\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5427 non-null   object\n",
      " 1   labels  5427 non-null   object\n",
      " 2   id      5427 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 127.3+ KB\n",
      "\n",
      "Sample Data:                                                 text labels       id\n",
      "0  I’m really sorry about your situation :( Altho...   [25]  eecwqtt\n",
      "1    It's wonderful because it's awful. At not with.    [0]  ed5f85d\n",
      "2  Kings fan here, good luck to you guys! Will be...   [13]  een27c3\n",
      "3  I didn't know that, thank you for teaching me ...   [15]  eelgwd1\n",
      "4  They got bored from haunting earth for thousan...   [27]  eem5uti \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas DataFrames\n",
    "train_df = train_data.to_pandas()\n",
    "validation_df = validation_data.to_pandas()\n",
    "test_df = test_data.to_pandas()\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(\"Training Data Info:\")\n",
    "train_df.info()\n",
    "print(\"\\nSample Data:\", train_df.head(), \"\\n\\n\\n\")\n",
    "\n",
    "print(\"Validation Data Info:\")\n",
    "validation_df.info()\n",
    "print(\"\\nSample Data:\", validation_df.head(), \"\\n\\n\\n\")\n",
    "\n",
    "print(\"Testing Data Info:\")\n",
    "test_df.info()\n",
    "print(\"\\nSample Data:\", test_df.head(), \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "806mUtcpr3uF",
    "outputId": "e2310d01-bd36-49cb-91cf-7f1f33e80ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "\n",
      "Training data frame: \n",
      " text      0\n",
      "labels    0\n",
      "id        0\n",
      "dtype: int64\n",
      "\n",
      "Validation data frame: \n",
      " text      0\n",
      "labels    0\n",
      "id        0\n",
      "dtype: int64\n",
      "\n",
      "Testing data frame: \n",
      " text      0\n",
      "labels    0\n",
      "id        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for null values\n",
    "print(\"Missing Values:\")\n",
    "print(\"\\nTraining data frame: \\n\", train_df.isnull().sum())\n",
    "print(\"\\nValidation data frame: \\n\", train_df.isnull().sum())\n",
    "print(\"\\nTesting data frame: \\n\", train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPvifbLAsEzW",
    "outputId": "f99eb165-e322-4388-e3e4-8adfbf306d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Training Data Shape: (36308, 3)\n",
      "\n",
      "Filtered Validation Data Shape: (4548, 3)\n",
      "\n",
      "Filtered Testing Data Shape: (4590, 3)\n",
      "\n",
      "Sample from Training Data:\n",
      "                                                 text  labels       id\n",
      "0  My favourite food is anything I didn't have to...      27  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...      27  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING       2  eezlygj\n",
      "3                        To make her feel threatened      14  ed7ypvh\n",
      "4                             Dirty Southern Wankers       3  ed0bdzj\n",
      "\n",
      "Sample from Validation Data:\n",
      "                                                 text  labels       id\n",
      "0  Is this in New Orleans?? I really feel like th...      27  edgurhb\n",
      "2               I've never been this sad in my life!      25  edcu99z\n",
      "4  He could have easily taken a real camera from ...      20  eepig6r\n",
      "5  Thank you for your vote of confidence, but we ...      15  eczm50f\n",
      "6  Wah Mum other people call me on my bullshit an...       2  ed4yr9r\n",
      "\n",
      "Sample from Test Data:\n",
      "                                                 text  labels       id\n",
      "0  I’m really sorry about your situation :( Altho...      25  eecwqtt\n",
      "1    It's wonderful because it's awful. At not with.       0  ed5f85d\n",
      "2  Kings fan here, good luck to you guys! Will be...      13  een27c3\n",
      "3  I didn't know that, thank you for teaching me ...      15  eelgwd1\n",
      "4  They got bored from haunting earth for thousan...      27  eem5uti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\finnl_y\\AppData\\Local\\Temp\\ipykernel_4936\\1964841269.py:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  train_df.loc[:, 'labels'] = train_df['labels'].apply(extract_label)\n",
      "C:\\Users\\finnl_y\\AppData\\Local\\Temp\\ipykernel_4936\\1964841269.py:12: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  validation_df.loc[:, 'labels'] = validation_df['labels'].apply(extract_label)\n",
      "C:\\Users\\finnl_y\\AppData\\Local\\Temp\\ipykernel_4936\\1964841269.py:15: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  test_df.loc[:, 'labels'] = test_df['labels'].apply(extract_label)\n"
     ]
    }
   ],
   "source": [
    "# Delete multiple labels\n",
    "\n",
    "# Define a helper function to extract the single label\n",
    "def extract_label(label_list):\n",
    "  return label_list[0]\n",
    "\n",
    "# Filter rows with exactly one label and extract the single label\n",
    "train_df = train_df[train_df['labels'].apply(len) == 1]\n",
    "train_df.loc[:, 'labels'] = train_df['labels'].apply(extract_label)\n",
    "\n",
    "validation_df = validation_df[validation_df['labels'].apply(len) == 1]\n",
    "validation_df.loc[:, 'labels'] = validation_df['labels'].apply(extract_label)\n",
    "\n",
    "test_df = test_df[test_df['labels'].apply(len) == 1]\n",
    "test_df.loc[:, 'labels'] = test_df['labels'].apply(extract_label)\n",
    "\n",
    "# Check shapes after filtering\n",
    "print(\"Filtered Training Data Shape:\", train_df.shape)\n",
    "print(\"\\nFiltered Validation Data Shape:\", validation_df.shape)\n",
    "print(\"\\nFiltered Testing Data Shape:\", test_df.shape)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nSample from Training Data:\\n\", train_df.head())\n",
    "\n",
    "print(\"\\nSample from Validation Data:\\n\", validation_df.head())\n",
    "\n",
    "print(\"\\nSample from Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIy5bzn06gy-",
    "outputId": "c1c9b5c2-dbe8-49e5-837e-ce8e0000ed84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27  2 14  3 26 15  0  6  5 12 17 25 10 20  4 13  1  9 24 18  7 22 11 23\n",
      " 21 16  8 19]\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['labels'].unique())  # Show all unique values in the column\n",
    "print(train_df['labels'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aK2hDarR62IV",
    "outputId": "1be59bab-cf2e-4d1a-eb30-6360752bff1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "# Conversion of label objects to Integers\n",
    "train_df['labels'] = train_df['labels'].astype(int)\n",
    "validation_df['labels'] = validation_df['labels'].astype(int)\n",
    "test_df['labels'] = test_df['labels'].astype(int)\n",
    "\n",
    "# Confirm the data type\n",
    "print(train_df['labels'].dtype)  # Should now show 'int64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sXEZuXb7JqK",
    "outputId": "f2bab71e-d354-4308-a082-d029b8b2d43d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Training Data Shape: (36308, 3)\n",
      "\n",
      "Filtered Validation Data Shape: (4548, 3)\n",
      "\n",
      "Filtered Testing Data Shape: (4590, 3)\n",
      "\n",
      "Sample from Training Data:\n",
      "                                                 text  labels       id\n",
      "0  My favourite food is anything I didn't have to...      27  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...      27  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING       2  eezlygj\n",
      "3                        To make her feel threatened      14  ed7ypvh\n",
      "4                             Dirty Southern Wankers       3  ed0bdzj\n",
      "\n",
      "Sample from Validation Data:\n",
      "                                                 text  labels       id\n",
      "0  Is this in New Orleans?? I really feel like th...      27  edgurhb\n",
      "2               I've never been this sad in my life!      25  edcu99z\n",
      "4  He could have easily taken a real camera from ...      20  eepig6r\n",
      "5  Thank you for your vote of confidence, but we ...      15  eczm50f\n",
      "6  Wah Mum other people call me on my bullshit an...       2  ed4yr9r\n",
      "\n",
      "Sample from Test Data:\n",
      "                                                 text  labels       id\n",
      "0  I’m really sorry about your situation :( Altho...      25  eecwqtt\n",
      "1    It's wonderful because it's awful. At not with.       0  ed5f85d\n",
      "2  Kings fan here, good luck to you guys! Will be...      13  een27c3\n",
      "3  I didn't know that, thank you for teaching me ...      15  eelgwd1\n",
      "4  They got bored from haunting earth for thousan...      27  eem5uti\n"
     ]
    }
   ],
   "source": [
    "# Final check\n",
    "# Check shapes after filtering\n",
    "print(\"Filtered Training Data Shape:\", train_df.shape)\n",
    "print(\"\\nFiltered Validation Data Shape:\", validation_df.shape)\n",
    "print(\"\\nFiltered Testing Data Shape:\", test_df.shape)\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nSample from Training Data:\\n\", train_df.head())\n",
    "\n",
    "print(\"\\nSample from Validation Data:\\n\", validation_df.head())\n",
    "\n",
    "print(\"\\nSample from Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-hZo9fywtpC",
    "outputId": "360fbe21-1f9d-4447-ec43-d7ed0c568a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training Feature Matrix Shape: (36308, 24311)\n",
      "Baseline Validation Feature Matrix Shape: (4548, 24311)\n",
      "Baseline Testing Feature Matrix Shape: (4590, 24311)\n",
      "Baseline Training label Matrix Shape: (36308,)\n",
      "Baseline Validation label Matrix Shape: (4548,)\n",
      "Baseline Testing label Matrix Shape: (4590,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for Baseline\n",
    "\n",
    "# Initialize a Vectorizer\n",
    "#vectorizer_baseline = CountVectorizer()\n",
    "vectorizer_baseline = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on training data and transform all splits\n",
    "X_train_baseline = vectorizer_baseline.fit_transform(train_df['text'])\n",
    "X_validation_baseline = vectorizer_baseline.transform(validation_df['text'])\n",
    "X_test_baseline = vectorizer_baseline.transform(test_df['text'])\n",
    "\n",
    "# Labels (already preprocessed)\n",
    "y_train_baseline = train_df['labels'].to_numpy()  # Convert to NumPy array\n",
    "y_validation_baseline = validation_df['labels'].to_numpy()\n",
    "y_test_baseline = test_df['labels'].to_numpy()\n",
    "\n",
    "# Verify feature matrix shapes\n",
    "print(\"Baseline Training Feature Matrix Shape:\", X_train_baseline.shape)\n",
    "print(\"Baseline Validation Feature Matrix Shape:\", X_validation_baseline.shape)\n",
    "print(\"Baseline Testing Feature Matrix Shape:\", X_test_baseline.shape)\n",
    "print(\"Baseline Training label Matrix Shape:\", y_train_baseline.shape)\n",
    "print(\"Baseline Validation label Matrix Shape:\", y_validation_baseline.shape)\n",
    "print(\"Baseline Testing label Matrix Shape:\", y_test_baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "va0hr5JV2ry2",
    "outputId": "edede2cb-b5b2-4c61-95f1-7f1459d5868a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Feature Matrix Shape: (36308, 24311)\n",
      "Naive Bayes Validation Feature Matrix Shape: (4548, 24311)\n",
      "Naive Bayes Testing Feature Matrix Shape: (4590, 24311)\n",
      "Naive Bayes Training label Matrix Shape: (36308,)\n",
      "Naive Bayes Validation label Matrix Shape: (4548,)\n",
      "Naive Bayes Testing label Matrix Shape: (4590,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for Bayesian\n",
    "\n",
    "# Initialize a Vectorizer\n",
    "vectorizer_bayesian = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on training data and transform all splits\n",
    "X_train_bayesian = vectorizer_bayesian.fit_transform(train_df['text'])\n",
    "X_validation_bayesian = vectorizer_bayesian.transform(validation_df['text'])\n",
    "X_test_bayesian = vectorizer_bayesian.transform(test_df['text'])\n",
    "\n",
    "# Labels (already preprocessed)\n",
    "y_train_bayesian = train_df['labels'].to_numpy()  # Convert to NumPy array\n",
    "y_validation_bayesian = validation_df['labels'].to_numpy()\n",
    "y_test_bayesian = test_df['labels'].to_numpy()\n",
    "\n",
    "# Verify feature matrix shapes\n",
    "print(\"Naive Bayes Training Feature Matrix Shape:\", X_train_bayesian.shape)\n",
    "print(\"Naive Bayes Validation Feature Matrix Shape:\", X_validation_bayesian.shape)\n",
    "print(\"Naive Bayes Testing Feature Matrix Shape:\", X_test_bayesian.shape)\n",
    "print(\"Naive Bayes Training label Matrix Shape:\", y_train_bayesian.shape)\n",
    "print(\"Naive Bayes Validation label Matrix Shape:\", y_validation_bayesian.shape)\n",
    "print(\"Naive Bayes Testing label Matrix Shape:\", y_test_bayesian.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6jXPOP1ysA4",
    "outputId": "c6c9765f-63cf-43aa-d31c-e50b0c12e16f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution: \n",
      "     Class Label  Count  Frequency (%)\n",
      "0             0   2710       7.463920\n",
      "1             1   1652       4.549961\n",
      "2             2   1025       2.823069\n",
      "3             3   1451       3.996364\n",
      "4             4   1873       5.158643\n",
      "5             5    649       1.787485\n",
      "6             6    858       2.363116\n",
      "7             7   1389       3.825603\n",
      "8             8    389       1.071389\n",
      "9             9    709       1.952738\n",
      "10           10   1402       3.861408\n",
      "11           11    498       1.371599\n",
      "12           12    203       0.559105\n",
      "13           13    510       1.404649\n",
      "14           14    430       1.184312\n",
      "15           15   1857       5.114575\n",
      "16           16     39       0.107414\n",
      "17           17    853       2.349344\n",
      "18           18   1427       3.930263\n",
      "19           19     85       0.234108\n",
      "20           20    861       2.371378\n",
      "21           21     51       0.140465\n",
      "22           22    586       1.613969\n",
      "23           23     88       0.242371\n",
      "24           24    353       0.972238\n",
      "25           25    817       2.250193\n",
      "26           26    720       1.983034\n",
      "27           27  12823      35.317285\n",
      "\n",
      "Validation Label Distribution:\n",
      "     Class Label  Count  Frequency (%)\n",
      "0             0    326       7.167986\n",
      "1             1    208       4.573439\n",
      "2             2    109       2.396658\n",
      "3             3    164       3.605981\n",
      "4             4    258       5.672823\n",
      "5             5     96       2.110818\n",
      "6             6    102       2.242744\n",
      "7             7    164       3.605981\n",
      "8             8     52       1.143360\n",
      "9             9     91       2.000880\n",
      "10           10    212       4.661390\n",
      "11           11     61       1.341249\n",
      "12           12     20       0.439754\n",
      "13           13     52       1.143360\n",
      "14           14     58       1.275286\n",
      "15           15    261       5.738786\n",
      "16           16      6       0.131926\n",
      "17           17    106       2.330695\n",
      "18           18    173       3.803870\n",
      "19           19      8       0.175901\n",
      "20           20    119       2.616535\n",
      "21           21      9       0.197889\n",
      "22           22     74       1.627089\n",
      "23           23      8       0.175901\n",
      "24           24     40       0.879507\n",
      "25           25     84       1.846966\n",
      "26           26     95       2.088830\n",
      "27           27   1592      35.004398\n",
      "\n",
      "Testing Label Distribution:\n",
      "     Class Label  Count  Frequency (%)\n",
      "0             0    348       7.581699\n",
      "1             1    186       4.052288\n",
      "2             2    131       2.854031\n",
      "3             3    194       4.226580\n",
      "4             4    236       5.141612\n",
      "5             5     86       1.873638\n",
      "6             6     97       2.113290\n",
      "7             7    176       3.834423\n",
      "8             8     56       1.220044\n",
      "9             9     88       1.917211\n",
      "10           10    195       4.248366\n",
      "11           11     76       1.655773\n",
      "12           12     23       0.501089\n",
      "13           13     57       1.241830\n",
      "14           14     65       1.416122\n",
      "15           15    260       5.664488\n",
      "16           16      2       0.043573\n",
      "17           17     93       2.026144\n",
      "18           18    160       3.485839\n",
      "19           19     12       0.261438\n",
      "20           20    107       2.331155\n",
      "21           21      7       0.152505\n",
      "22           22     89       1.938998\n",
      "23           23      7       0.152505\n",
      "24           24     44       0.958606\n",
      "25           25    102       2.222222\n",
      "26           26     87       1.895425\n",
      "27           27   1606      34.989107\n"
     ]
    }
   ],
   "source": [
    "# Class distributions\n",
    "\n",
    "def class_distribtion(label_set):\n",
    "  labels, counts = np.unique(label_set, return_counts=True)\n",
    "\n",
    "  # Calculate frequencies as percentages\n",
    "  frequencies = (counts / counts.sum()) * 100\n",
    "\n",
    "  # Create a DataFrame to store counts and frequencies\n",
    "  class_distribution_table = pd.DataFrame({\n",
    "    \"Class Label\": labels,\n",
    "    \"Count\": counts,\n",
    "    \"Frequency (%)\": frequencies\n",
    "  })\n",
    "\n",
    "  # Sort the table by class label for better organization\n",
    "  class_distribution_table = class_distribution_table.sort_values(by=\"Class Label\").reset_index(drop=True)\n",
    "  return class_distribution_table\n",
    "\n",
    "print(\"Training Label Distribution: \\n\", class_distribtion(y_train_bayesian))\n",
    "print(\"\\nValidation Label Distribution:\\n\", class_distribtion(y_validation_bayesian))\n",
    "print(\"\\nTesting Label Distribution:\\n\", class_distribtion(y_test_bayesian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIy6Zvif3iwz",
    "outputId": "8051f9e1-af3b-484a-adbb-68283095798f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Input IDs Shape: torch.Size([36308, 128])\n",
      "Training Attention Masks Shape: torch.Size([36308, 128])\n",
      "Training Labels Shape: torch.Size([36308])\n",
      "Validation Input IDs Shape: torch.Size([4548, 128])\n",
      "Validation Attention Masks Shape: torch.Size([4548, 128])\n",
      "Validation Labels Shape: torch.Size([4548])\n",
      "Testing Input IDs Shape: torch.Size([4590, 128])\n",
      "Testing Attention Masks Shape: torch.Size([4590, 128])\n",
      "Testing Labels Shape: torch.Size([4590])\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for LLM\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # We will use bert-base-uncased\n",
    "\n",
    "# Tokenize text data for all splits\n",
    "train_encodings = tokenizer_llm(\n",
    "    list(train_df['text']), truncation=True, padding='max_length', max_length=128\n",
    ")\n",
    "validation_encodings = tokenizer_llm(\n",
    "    list(validation_df['text']), truncation=True, padding='max_length', max_length=128\n",
    ")\n",
    "test_encodings = tokenizer_llm(\n",
    "    list(test_df['text']), truncation=True, padding='max_length', max_length=128\n",
    ")\n",
    "\n",
    "# Convert tokenized data into PyTorch tensors\n",
    "X_train_llm = torch.tensor(train_encodings['input_ids'])\n",
    "X_train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "y_train_llm = torch.tensor(train_df['labels'].to_numpy())\n",
    "\n",
    "X_validation_llm = torch.tensor(validation_encodings['input_ids'])\n",
    "X_validation_attention_masks = torch.tensor(validation_encodings['attention_mask'])\n",
    "y_validation_llm = torch.tensor(validation_df['labels'].to_numpy())\n",
    "\n",
    "X_test_llm = torch.tensor(test_encodings['input_ids'])\n",
    "X_test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "y_test_llm = torch.tensor(test_df['labels'].to_numpy())\n",
    "\n",
    "# Check shapes\n",
    "print(\"Training Input IDs Shape:\", X_train_llm.shape)\n",
    "print(\"Training Attention Masks Shape:\", X_train_attention_masks.shape)\n",
    "print(\"Training Labels Shape:\", y_train_llm.shape)\n",
    "\n",
    "print(\"Validation Input IDs Shape:\", X_validation_llm.shape)\n",
    "print(\"Validation Attention Masks Shape:\", X_validation_attention_masks.shape)\n",
    "print(\"Validation Labels Shape:\", y_validation_llm.shape)\n",
    "\n",
    "print(\"Testing Input IDs Shape:\", X_test_llm.shape)\n",
    "print(\"Testing Attention Masks Shape:\", X_test_attention_masks.shape)\n",
    "print(\"Testing Labels Shape:\", y_test_llm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Implement Naive Bayes and Finetune an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 - Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(Z):                                                # dimension C x N\n",
    "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
    "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
    "    return log_sum_exp\n",
    "\n",
    "class NaiveBayes: \n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        N, D = x.shape\n",
    "        C = np.max(y) + 1\n",
    "        # one parameter for each feature conditioned on each class\n",
    "        mu, sigma = np.zeros((C,D)), np.zeros((C,D))\n",
    "        Nc = np.zeros(C) # number of instances in class c\n",
    "        # for each class get the MLE for the mean and std\n",
    "        for c in range(C):\n",
    "            x_c = x[y == c]                           #slice all the elements from class c\n",
    "            Nc[c] = x_c.shape[0]                      #get number of elements of class c\n",
    "            mu[c,:] = np.mean(x_c,0)                  #mean of features of class c\n",
    "            sigma[c,:] = np.std(x_c, 0)               #std of features of class c\n",
    "            \n",
    "        self.mu = mu                                  # C x D\n",
    "        self.sigma = sigma                            # C x D\n",
    "        self.pi = (Nc+1)/(N+C)                        #Laplace smoothing (using alpha_c=1 for all c) you can derive using Dirichlet's distribution\n",
    "        return self\n",
    "\n",
    "    def predict(self, xt):\n",
    "        Nt, D = xt.shape\n",
    "        # for numerical stability we work in the log domain\n",
    "        # we add a dimension because this is added to the log-likelihood matrix \n",
    "        # that assigns a likelihood for each class (C) to each test point, and so it is C x N\n",
    "        log_prior = np.log(self.pi)[:, None]\n",
    "        # logarithm of the likelihood term for Gaussian \n",
    "        # the first two terms are the logarithm of the normalization term in the Gaussian and the final term is the exponent in the Gaussian. \n",
    "        # Notice that we are adding dimensions (using None) to model parameters and data to make this evaluation. \n",
    "        # The reason is that sigma and mu are C x D, while the data x is N x D. We operate on a C x N x D shape by increasing the number of dimensions when needed\n",
    "        log_likelihood = -.5 * np.log(2*np.pi) - np.log(self.sigma[:,None,:]) -.5 * (((xt[None,:,:] - self.mu[:,None,:])/self.sigma[:,None,:])**2)\n",
    "        # now we sum over the feature dimension to get a C x N matrix (this has the log-likelihood for each class-test point combination)\n",
    "        log_likelihood = np.sum(log_likelihood, axis=2)\n",
    "        # posterior calculation\n",
    "        log_posterior = log_prior + log_likelihood\n",
    "        posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
    "        return posterior.T                                                  # dimension N x C\n",
    "    \n",
    "    def evaluate_acc(self, y, y_hat):\n",
    "        return np.mean(y == y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayesOptimized:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.feature_likelihoods = {}\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Calculate prior probabilities\n",
    "        for cls in self.classes:\n",
    "            X_cls = X[y == cls]\n",
    "            self.class_priors[cls] = X_cls.shape[0] / n_samples\n",
    "            self.feature_likelihoods[cls] = {}\n",
    "            \n",
    "            # Calculate likelihoods\n",
    "            for feature_index in range(n_features):\n",
    "                feature_values = X_cls[:, feature_index]\n",
    "                unique_values, counts = np.unique(feature_values, return_counts=True)\n",
    "                likelihoods = counts / counts.sum()\n",
    "                self.feature_likelihoods[cls][feature_index] = dict(zip(unique_values, likelihoods))\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        log_probs = np.zeros((n_samples, len(self.classes)))\n",
    "        \n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            log_prob = np.log(self.class_priors[cls])\n",
    "            log_probs[:, idx] = log_prob\n",
    "            \n",
    "            for feature_index in range(n_features):\n",
    "                feature_values = X[:, feature_index]\n",
    "                likelihoods = np.array([self.feature_likelihoods[cls][feature_index].get(value, 1e-6) for value in feature_values])\n",
    "                log_probs[:, idx] += np.log(likelihoods)\n",
    "        \n",
    "        return self.classes[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "    def evaluate_acc(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return correct / len(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m naive_bayes \u001b[38;5;241m=\u001b[39m NaiveBayesOptimized() \n\u001b[0;32m     11\u001b[0m naive_bayes\u001b[38;5;241m.\u001b[39mfit(X_train_bayesian\u001b[38;5;241m.\u001b[39mtoarray(), y_train_bayesian)\n\u001b[1;32m---> 12\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnaive_bayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_bayesian\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     15\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m naive_bayes\u001b[38;5;241m.\u001b[39mevaluate_acc(y_test_bayesian, y_hat)\n",
      "Cell \u001b[1;32mIn[183], line 37\u001b[0m, in \u001b[0;36mNaiveBayesOptimized.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     35\u001b[0m         feature_values \u001b[38;5;241m=\u001b[39m X[:, feature_index]\n\u001b[0;32m     36\u001b[0m         likelihoods \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_likelihoods[\u001b[38;5;28mcls\u001b[39m][feature_index]\u001b[38;5;241m.\u001b[39mget(value, \u001b[38;5;241m1e-6\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m feature_values])\n\u001b[1;32m---> 37\u001b[0m         log_probs[:, idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlikelihoods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses[np\u001b[38;5;241m.\u001b[39margmax(log_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_small = X_train_bayesian[:500]\n",
    "y_train_small = y_train_bayesian[:500]\n",
    "\n",
    "X_validation_small = X_validation_bayesian[:500]\n",
    "y_validation_small = y_validation_bayesian[:500]\n",
    "\n",
    "X_test_small = X_test_bayesian[:500]\n",
    "y_test_small = y_test_bayesian[:500]\n",
    "\n",
    "naive_bayes = NaiveBayesOptimized() \n",
    "naive_bayes.fit(X_train_bayesian.toarray(), y_train_bayesian)\n",
    "y_hat = naive_bayes.predict(X_test_bayesian.toarray())\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = naive_bayes.evaluate_acc(y_test_bayesian, y_hat)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Softmax Regression): 0.5521108179419525\n",
      "Test Accuracy (Softmax Regression): 0.5490196078431373\n"
     ]
    }
   ],
   "source": [
    "# 2. Implement Softmax Regression from Sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the model\n",
    "softmax_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Fit the model\n",
    "softmax_regression.fit(X_train_baseline, y_train_baseline)\n",
    "\n",
    "# Predict on the validation, test set\n",
    "y_pred_softmax = softmax_regression.predict(X_validation_baseline)\n",
    "y_pred_test_softmax = softmax_regression.predict(X_test_baseline)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_validation_softmax = accuracy_score(y_validation_baseline, y_pred_softmax)\n",
    "accuracy_test_softmax = accuracy_score(y_test_baseline, y_pred_test_softmax)\n",
    "\n",
    "print(\"Validation Accuracy (Softmax Regression):\", accuracy_validation_softmax)\n",
    "print(\"Test Accuracy (Softmax Regression):\", accuracy_test_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "require.config({\n    paths: {\n        d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n        jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n    }\n});\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "    paths: {\n",
    "        d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n",
    "        jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "    }\n",
    "});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\n\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\finnl_y\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe\"\n  * The NumPy version is: \"1.24.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\_core\\__init__.py:23\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiarray\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\_core\\multiarray.py:10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m overrides\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _multiarray_umath\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\_core\\overrides.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getargspec\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiarray_umath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     add_docstring,  _get_implementing_args, _ArrayFunctionDispatcher)\n\u001b[0;32m     12\u001b[0m ARRAY_FUNCTIONS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\utils\\import_utils.py:1778\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\bert\\modeling_bert.py:47\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     38\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     TokenClassifierOutput,\n\u001b[0;32m     46\u001b[0m )\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\modeling_utils.py:48\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOSS_MAPPING\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     Conv1D,\n\u001b[0;32m     51\u001b[0m     apply_chunking_to_forward,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     prune_linear_layer,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\loss\\loss_utils.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BCEWithLogitsLoss, MSELoss\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_deformable_detr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ForObjectDetectionLoss, ForSegmentationLoss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\loss\\loss_deformable_detr.py:6\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scipy_available\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloss_for_object_detection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     HungarianMatcher,\n\u001b[0;32m      8\u001b[0m     ImageLoss,\n\u001b[0;32m      9\u001b[0m     _set_aux_loss,\n\u001b[0;32m     10\u001b[0m     generalized_box_iou,\n\u001b[0;32m     11\u001b[0m     sigmoid_focal_loss,\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scipy_available():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\loss\\loss_for_object_detection.py:28\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scipy_available():\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_sum_assignment\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\__init__.py:405\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 405\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_minimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_root\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_minimize.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trustregion_exact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_trustregion_exact\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_trustregion_constr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# constrained minimization\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_trustregion_constr\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This module contains the equality constrained SQP solver.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminimize_trustregion_constr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimize_trustregion_constr\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_minimize_trustregion_constr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_trustregion_constr\\minimize_trustregion_constr.py:5\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_differentiable_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorFunction\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constraints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hessian_update_strategy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BFGS\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\optimize\\_constraints.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn, catch_warnings, simplefilter\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suppress_warnings\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\testing\\__init__.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\testing\\_private\\utils.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isfinite, isnan, isinf\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\_core\\__init__.py:49\u001b[0m\n\u001b[0;32m     26\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124mIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m0\u001b[39m], sys\u001b[38;5;241m.\u001b[39mversion_info[\u001b[38;5;241m1\u001b[39m], sys\u001b[38;5;241m.\u001b[39mexecutable,\n\u001b[0;32m     48\u001b[0m         __version__, exc)\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\finnl_y\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe\"\n  * The NumPy version is: \"1.24.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# this is the tokenizer using BERT tokenizer_llm\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model_llm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    560\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    561\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m--> 563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:388\u001b[0m, in \u001b[0;36m_get_model_class\u001b[1;34m(config, model_mapping)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[1;32m--> 388\u001b[0m     supported_models \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:763\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping:\n\u001b[0;32m    762\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping[model_type]\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m model_types \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config_mapping\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m key\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:777\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[1;34m(self, model_type, attr)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[module_name] \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\models\\auto\\auto_factory.py:693\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[1;34m(module, attr)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attr)\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, attr)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# object at the top level.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\utils\\import_utils.py:1766\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1766\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\transformers\\utils\\import_utils.py:1780\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1783\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\n\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.9 from \"C:\\Users\\finnl_y\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe\"\n  * The NumPy version is: \"1.24.1\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n"
     ]
    }
   ],
   "source": [
    "#LLM Model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# this is the tokenizer using BERT tokenizer_llm\n",
    "\n",
    "model_llm = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#freeze all parameters except the last layer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m      3\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#the classification head (last layer) remains trainable\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "#freeze all parameters except the last layer\n",
    "for param in model_llm.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#the classification head (last layer) remains trainable\n",
    "for param in model_llm.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], y_train_llm)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "validation_dataset = TensorDataset(validation_encodings['input_ids'], validation_encodings['attention_mask'], y_validation_llm)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16)\n",
    "\n",
    "#ensure optimizer only works on trainable layers\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model_llm.parameters()), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_llm.to(device)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model_llm.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_llm(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # CrossEntropyLoss is handled internally\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on validation set\n",
    "\n",
    "model_llm.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in validation_loader:\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        \n",
    "        outputs = model_llm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
