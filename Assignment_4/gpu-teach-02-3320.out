Loading model...
Model loaded successfully.
Loading tokenizer...
Tokenizing inputs...
Tokenized inputs: {'input_ids': tensor([[  101,  1996,  3185,  2001, 10392,  1998,  2440,  1997, 20096,  1012,
           102,  1045, 12246,  5632,  1996,  2143,  1012,   102]],
       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]],
       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
       device='cuda:0')}
Running forward pass to compute attention...
Attention computed successfully.
Tokens: ['[CLS]', 'the', 'movie', 'was', 'fantastic', 'and', 'full', 'of', 'surprises', '.', '[SEP]', 'i', 'thoroughly', 'enjoyed', 'the', 'film', '.', '[SEP]']
Sentence B start index: 11
Generating head view visualization...
<IPython.core.display.HTML object>
<IPython.core.display.HTML object>
<IPython.core.display.Javascript object>
An error occurred during BERTViz testing: head_view returned None, visualization could not be generated.
